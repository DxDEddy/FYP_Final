{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04c8a04b",
   "metadata": {},
   "source": [
    "# Multi-Classification Machine Learning for Malware Analysis\n",
    "## 9 Types of Malware in this dataset:\n",
    "1. Ramnit         - RAT\n",
    "2. Lollipop       - Adware\n",
    "3. Kelihos_ver3   - RAT\n",
    "4. Vundo          - Adware\n",
    "5. Simda          - Botnet\n",
    "6. Tracur         - Malicious Browser Plugin\n",
    "7. Kelihos_ver1   - RAT\n",
    "8. Obfuscator.ACY - Obfuscates other malware/information\n",
    "9. Gatak          - RAT\n",
    "\n",
    "## Game Plan:\n",
    "\n",
    "- Look into creating more metrics to show off my model\n",
    "- Improve the way I import data for the model\n",
    "- Explain my code and solution in detail\n",
    "- Port into the main program/script\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529f870",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b68674",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#pip install scikit-learn\n",
    "#pip install seaborn\n",
    "#pip install matplotlib\n",
    "#pip install pandas\n",
    "#pip install torch\n",
    "#pip install torchvision\n",
    "#pip install jupyterthemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678f78f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All imports centralised here\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "import heapq\n",
    "import codecs\n",
    "import json\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "from pathlib import Path #Convert all directory accesses to this\n",
    "from functools import reduce\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c088df",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9004337c",
   "metadata": {
    "code_folding": [
     0,
     3,
     9,
     14,
     18,
     22,
     25,
     30,
     38,
     44,
     51,
     57,
     65,
     68,
     74,
     84,
     90
    ]
   },
   "outputs": [],
   "source": [
    "# Functions are all contained inside here\n",
    "\n",
    "def createFullDirectory(mainDirectory,subDirectory):\n",
    "    return str(mainDirectory+subDirectory)\n",
    "\n",
    "def createFullPathToFile(fullDirectory, fileName):\n",
    "    return str(fullDirectory+fileName)\n",
    "\n",
    "def listFilesInDirectory(directoryContainingFiles):\n",
    "    return glob.glob(directoryContainingFiles) \n",
    "\n",
    "def stripFilePathAndExtension(filePath, prefixToStrip, suffixToStrip):\n",
    "    filePath = filePath.replace(prefixToStrip, \"\")\n",
    "    filePath = filePath.replace(suffixToStrip, \"\")\n",
    "    #return filePath\n",
    "    return Path(filePath).stem\n",
    "\n",
    "def replaceFilePathAndExtension(filePath, prefixToStrip, prefixToInsert, suffixToStrip, suffixToInsert):\n",
    "    filePath = filePath.replace(prefixToStrip, prefixToInsert)\n",
    "    filePath = filePath.replace(suffixToStrip, suffixToInsert)\n",
    "    return filePath\n",
    "\n",
    "def printDataFrame(dataframe):\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        print(dataframe)\n",
    "\n",
    "def zeroOutDataframe(dataframe):\n",
    "    dataframe = dataframe.fillna(0)\n",
    "    return dataframe\n",
    "\n",
    "def countEntriesInDataframe(dataframe):\n",
    "    return np.count_nonzero(dataframe)\n",
    "\n",
    "def sortDictionary(dictionary):\n",
    "    returnVal = sorted(dict(Counter(dictionary)).items(), key=lambda kv:\n",
    "                 (kv[1], kv[0]))\n",
    "    return returnVal\n",
    "\n",
    "def fileNewlineIntoList(filePath):\n",
    "    lineList = []\n",
    "    with open(filePath) as openFile:\n",
    "        for line in openFile:\n",
    "            temp = line.strip()\n",
    "            lineList.append(temp)\n",
    "    return lineList\n",
    "\n",
    "def stripNewlineAndWhitespace(textStringToStrip):\n",
    "    textStringToStrip = textStringToStrip.replace(\"\\t\",\"\")\n",
    "    textStringToStrip = textStringToStrip.replace(\"\\n\",\"\")\n",
    "    textStringToStrip = textStringToStrip.replace(\" \",\"\")\n",
    "    return textStringToStrip\n",
    "\n",
    "def stripNewlineAndWhitespaceFromList(listToStrip):\n",
    "    for i in range(0,len(listToStrip)):\n",
    "        listToStrip[i] = listToStrip[i].replace(\"\\t\",\"\")\n",
    "        listToStrip[i] = listToStrip[i].replace(\"\\n\",\"\")\n",
    "        listToStrip[i] = listToStrip[i].replace(\" \",\"\")\n",
    "    return listToStrip\n",
    "\n",
    "def regexSearchFile(filePath, regexPattern):\n",
    "    with open(filePath) as openFile:\n",
    "        matches = re.findall(regexPattern, openFile.read())\n",
    "    openFile.close()\n",
    "    return matches\n",
    "\n",
    "def cleanFileNameList(fileNameList,malwareClass, sortedDatasetDirectory): #NEED TO PORT THIS\n",
    "    filePathToNameDict = {}\n",
    "    for i in range(0, len(fileNameList)): \n",
    "        strippedFile = stripFilePathAndExtension(fileNameList[i], sortedDatasetDirectory+\"/class-\"+str(malwareClass)+\"/\", \".asm\") #FIX THIS TO ALLOW FOR DIFFERENT CLASSES\n",
    "        filePathToNameDict[strippedFile] = fileNameList[i]\n",
    "        fileNameList[i] = strippedFile\n",
    "    return fileNameList\n",
    "\n",
    "def generateClassDataFrame(listColumnsToUse,listRowsToUse):\n",
    "    return zeroOutDataframe(pd.DataFrame(columns=listColumnsToUse,index=listRowsToUse))\n",
    "\n",
    "def moveFilesToClassFolders(backupFileList, fullFileNamesListFromCSV, unsortedDataset,sortedDataset): #Old and working before I tried the next version\n",
    "    fullFileNamesListFromCSV.set_index(\"Id\",inplace=True)\n",
    "    for fileIndex in range(0,len(backupFileList)): # file is the full path to the file, fileClean is just the name of the file without extension\n",
    "        fileClean = stripFilePathAndExtension(backupFileList[fileIndex],unsortedDataset,\".asm\")\n",
    "        try:\n",
    "            shutil.copyfile(backupFileList[fileIndex],sortedDataset+\"class-\"+str(fullFileNamesListFromCSV.loc[fileClean,\"Class\"])+\"/\"+str(fullFileNamesListFromCSV.loc[fileClean].name)+\".asm\")\n",
    "        except:\n",
    "            fileIndex = fileIndex + 1\n",
    "\n",
    "def generateFilenameToDirectoryDict(fileDirectory):\n",
    "    filePathToNameDict = {}\n",
    "    for file in fileDirectory:\n",
    "        filePathToNameDict[Path(file).stem] = file\n",
    "    return filePathToNameDict\n",
    "\n",
    "def populateMalwareDataframe(fileDirectoryTopLevel,instructionList):\n",
    "\n",
    "    filePathToNameDict = generateFilenameToDirectoryDict(listFilesInDirectory(fileDirectoryTopLevel))\n",
    "    dataFrame = zeroOutDataframe(pd.DataFrame(columns=instructionList,index=filePathToNameDict.keys()))\n",
    "\n",
    "    for file in filePathToNameDict.keys(): # Go through every file in our directory\n",
    "        fileDirectory = filePathToNameDict[file] # Convert using dict here\n",
    "        instructionsForThisFile = stripNewlineAndWhitespaceFromList(regexSearchFile(fileDirectory,\"(?:\\t{3,7}       (?!db|dd)[a-zA-Z]{2,6} {1,})\")) # cleaning and pulling instructions\n",
    "\n",
    "        pandasSeriesTest = pd.Series(instructionsForThisFile).value_counts().index, pd.Series(instructionsForThisFile).value_counts().values # Counting each instruction up   \n",
    "        for i in range(0, len(pandasSeriesTest[0])):\n",
    "            dataFrame.loc[file,pandasSeriesTest[0][i]] = pandasSeriesTest[1][i]  #0 = instruction and 1 = count columns ||| Second value is index within that column\n",
    "        \n",
    "        #Optional cleaning options for my DF to merge dupe columns and group them up\n",
    "        dataFrame = dataFrame.groupby(axis=1, level=0).sum() # Merges dupe columns\n",
    "        #dataFrame = dataFrame.loc[:, (dataFrame != 0).any(axis=0)] # Removes columns with no values\n",
    "    return dataFrame\n",
    "\n",
    "def classDataFrameCompletion(instructionList,sortedDataset,classList,classInteger):\n",
    "    print(sortedDataset+classList[classInteger-1]+\"/*.asm\")\n",
    "\n",
    "    \n",
    "    dataFrameInFunction = generateClassDataFrame(\n",
    "        instructionList,      # This is the instruction list\n",
    "        cleanFileNameList(    # This is the list of files\n",
    "            listFilesInDirectory(sortedDataset+classList[classInteger-1]+\"/*.asm\"),  # This is the directory containing the files\n",
    "            classInteger,\n",
    "            sortedDataset))  #This is the malware class for cleanFileNameList\n",
    "    \n",
    "    dataFrameInFunction = populateMalwareDataframe(\n",
    "                            sortedDataset+classList[classInteger-1]+\"/*.asm\",\n",
    "                            instructionList)\n",
    "\n",
    "    dataFrameInFunction = zeroOutDataframe(dataFrameInFunction)\n",
    "    \n",
    "    dataFrameInFunction.loc[~(dataFrameInFunction==0).all(axis=1)]\n",
    "    \n",
    "    dataFrameInFunction.insert(0,\"class\",classInteger)\n",
    "\n",
    "    print(sortedDataset+classList[classInteger-1]+\"/*.asm\")\n",
    "\n",
    "    return dataFrameInFunction\n",
    "\n",
    "def removeNanValuesFromDataframe(dataframeToSanitise):\n",
    "    dataframeToSanitise = dataframeToSanitise.replace(np.nan,0)\n",
    "    return dataframeToSanitise\n",
    "\n",
    "def normaliseData(dataframeToNormalise):\n",
    "    #return (data -trainStats[\"mean\"]) / trainStats['std'] #Works fine, experimenting with the OTHER\n",
    "    #return data.div(data.sum(axis=1), axis=0)\n",
    "\n",
    "    dataframeToNormalise = removeNanValuesFromDataframe(dataframeToNormalise)\n",
    "    return dataframeToNormalise\n",
    "    \n",
    "def modelSVMClassifierCreate(cValue, kernelType):\n",
    "    return svm.SVC(C=cValue, kernel=kernelType)\n",
    "    \n",
    "def svmModelFit(modelToFit,trainingDataframe, trainingDatasetLabels):\n",
    "    return modelToFit.fit(trainingDataframe, trainingDatasetLabels)\n",
    "\n",
    "def svmModelPredict(modelForPrediction, dataframeToPredictWith):\n",
    "    return modelForPrediction.predict(dataframeToPredictWith)\n",
    "\n",
    "def trainAndPredictModel(cValue, kernelType, trainingDataframe, trainingLabels):\n",
    "    model = modelSVMClassifierCreate(cValue, kernelType)\n",
    "    model = svmModelFit(model, trainingDataframe, trainingLabels)\n",
    "    modelPrediction = svmModelPredict(model, trainingDataframe)\n",
    "    return modelPrediction, model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eb09028",
   "metadata": {},
   "source": [
    "## Defining Directories and required structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3324b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory structures are defined here\n",
    "\n",
    "baseDirectory = \"/home/eddy/machine-learning/data/\"\n",
    "classList = [\"class-1\",\"class-2\",\"class-3\",\"class-4\",\"class-5\",\"class-6\",\"class-7\",\"class-8\",\"class-9\"]\n",
    "\n",
    "unsortedDataset = createFullDirectory(baseDirectory,\"dataset-training-full-sanitised/\")\n",
    "sortedDataset = createFullDirectory(baseDirectory,\"dataset-training-subset-sorted/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd2c1528",
   "metadata": {},
   "source": [
    "## Pulling the files from the dataset into the class folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8cd72",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Moving files from the santised but unsorted folder into the sanitised and sorted folder\n",
    "\n",
    "#moveFilesToClassFolders(listFilesInDirectory(unsortedDataset+\"*\"),pd.read_csv(\"/home/eddy/machine-learning/data/trainLabels.csv\"),unsortedDataset,sortedDataset)\n",
    "print(len(listFilesInDirectory(unsortedDataset+\"*\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3aaa811",
   "metadata": {},
   "source": [
    "### Making sure there are less than 250 files in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e08e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting all but 250 files in each class\n",
    "\n",
    "for fileClass in classList:\n",
    "    directory = str(sortedDataset+fileClass+\"/*\")\n",
    "    fileList = listFilesInDirectory(directory) #glob.glob(directory)\n",
    "\n",
    "    print(fileClass)\n",
    "    print(len(fileList))\n",
    "\n",
    "    i = 0\n",
    "    for i in range(0,len(fileList)):\n",
    "        if(i >= 250):\n",
    "            os.remove(fileList[i])\n",
    "    print(len(listFilesInDirectory(sortedDataset+classList[0]+\"/*.asm\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08339e72",
   "metadata": {},
   "source": [
    "## Creating the Pandas DataFrame for the malware classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dataframe for each of the classes\n",
    "\n",
    "instructionList = fileNewlineIntoList(\"/home/eddy/machine-learning/instructionListComplete.txt\")\n",
    "instructionList = [instruction.lower() for instruction in instructionList] # Making all instructions lowercase\n",
    "\n",
    "dataframeClassOne = classDataFrameCompletion(instructionList, sortedDataset, classList, 1)\n",
    "dataframeClassTwo = classDataFrameCompletion(instructionList, sortedDataset, classList, 2)\n",
    "dataframeClassThree = classDataFrameCompletion(instructionList, sortedDataset, classList, 3)\n",
    "dataframeClassFour = classDataFrameCompletion(instructionList, sortedDataset, classList, 4)\n",
    "dataframeClassFive = classDataFrameCompletion(instructionList, sortedDataset, classList, 5)\n",
    "dataframeClassSix = classDataFrameCompletion(instructionList, sortedDataset, classList, 6)\n",
    "dataframeClassSeven = classDataFrameCompletion(instructionList, sortedDataset, classList, 7)\n",
    "dataframeClassEight = classDataFrameCompletion(instructionList, sortedDataset, classList, 8)\n",
    "dataframeClassNine = classDataFrameCompletion(instructionList, sortedDataset, classList, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd53ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the Final Dataframe\n",
    "\n",
    "dataframesList = [dataframeClassOne,dataframeClassTwo,dataframeClassThree,dataframeClassFour,dataframeClassFive,dataframeClassSix,dataframeClassSeven,dataframeClassEight,dataframeClassNine]\n",
    "finalDF = pd.concat(dataframesList).drop_duplicates()\n",
    "finalDF = zeroOutDataframe(finalDF)\n",
    "finalDF.loc[~(finalDF==0).all(axis=1)]\n",
    "finalDF = finalDF.loc[:, (finalDF != 0).any(axis=0)] # Removes columns with no values\n",
    "finalDF = finalDF.sample(frac=1)\n",
    "finalDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09a03b",
   "metadata": {},
   "source": [
    "## Splitting the data into train+test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3795c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing up the dataset into train, validate and test sets\n",
    "trainDF, testAndValidDF = train_test_split(finalDF, test_size=0.4)\n",
    "testDF, validDF = train_test_split(testAndValidDF, test_size=0.5)\n",
    "\n",
    "print(f\"Training Dataset rows and columns: {trainDF.shape}\")\n",
    "print(f\"Test Dataset rows and columns: {testDF.shape}\")\n",
    "print(f\"Validation Dataset rows and columns: {validDF.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0874cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing training dataframe information and removing the classes to feed into the model\n",
    "trainStats = trainDF.describe()\n",
    "trainStats.pop(\"class\")\n",
    "#not doing sns here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15cda6a",
   "metadata": {},
   "source": [
    "## Training Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea71d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training stats based on the trainDF dataset\n",
    "trainStats = trainDF.describe()\n",
    "trainStats.pop(\"class\")\n",
    "trainStats = trainStats.transpose()\n",
    "trainStats.to_csv(\"/home/eddy/traindata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Training, Validation and Testing Labels and printing the length of each\n",
    "trainLabels = trainDF.pop(\"class\")\n",
    "print(\"Training data rows:    \"+str(len(trainLabels)))\n",
    "\n",
    "validLabels = validDF.pop(\"class\")\n",
    "print(\"Validation data rows:  \"+str(len(validLabels)))\n",
    "\n",
    "testLabels = testDF.pop(\"class\")\n",
    "print(\"Testing data rows:     \"+str(len(testLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d56d5",
   "metadata": {},
   "source": [
    "## Data Normalisation/Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6db494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation Functions happen here\n",
    "\n",
    "normalisedTrainDF = removeNanValuesFromDataframe(normaliseData(trainDF))\n",
    "normalisedValidDF = removeNanValuesFromDataframe(normaliseData(validDF))\n",
    "normalisedTestDF = removeNanValuesFromDataframe(normaliseData(testDF))\n",
    "\n",
    "normalisedTrainDF.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83f80820",
   "metadata": {},
   "source": [
    "## Training the model and creating a small prediction for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier Object is Created\n",
    "#model      = svm.SVC(C = 1.5, kernel='linear')\n",
    "#modelPoly  = svm.SVC(C = 1.5, kernel='poly')\n",
    "#modelRBF   = svm.SVC(C = 1.5, kernel='rbf')\n",
    "#modelSig   = svm.SVC(C = 1.5, kernel='sigmoid')\n",
    "\n",
    "#Train the model using the training sets\n",
    "#model.fit(normalisedTrainDF, trainLabels)\n",
    "#modelPoly.fit(normalisedTrainDF, trainLabels)\n",
    "#modelRBF.fit(normalisedTrainDF, trainLabels)\n",
    "#modelSig.fit(normalisedTrainDF, trainLabels)\n",
    "\n",
    "\n",
    "\n",
    "#Predict the response for test dataset\n",
    "#y_pred = model.predict(normalisedTrainDF)\n",
    "#svmPolyModelPrediction = modelPoly.predict(normalisedTrainDF)\n",
    "#svmRBFModelPrediction = modelRBF.predict(normalisedTrainDF)\n",
    "#svmSigmoidModelPrediction = modelSig.predict(normalisedTrainDF)\n",
    "\n",
    "y_pred, model = trainAndPredictModel(1.5, \"linear\", normalisedTrainDF, trainLabels)\n",
    "svmPolyModelPrediction, modelPoly = trainAndPredictModel(1.5, \"poly\", normalisedTrainDF, trainLabels)\n",
    "svmRBFModelPrediction, modelRBF = trainAndPredictModel(1.5, \"rbf\", normalisedTrainDF, trainLabels)\n",
    "svmSigmoidModelPrediction, modelSig = trainAndPredictModel(1.5, \"sigmoid\", normalisedTrainDF, trainLabels)\n",
    "\n",
    "\n",
    "\n",
    "print(y_pred)\n",
    "print(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b3893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the example prediction\n",
    "\n",
    "#exampleResultDF = generateClassDataframe([\"id\",\"prediction\",\"actualClass\"],)\n",
    "\n",
    "exampleResult = model.predict(normalisedTestDF[:10])\n",
    "\n",
    "print(pd.Series(list(normalisedTestDF[:10].index),index=exampleResult).to_string())\n",
    "print(f\"Predicted values: {exampleResult}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e426cde3",
   "metadata": {},
   "source": [
    "## Accuracy of training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for the Training Set\n",
    "\n",
    "#normalisedTrainDF = normalisedTrainDF[np.isfinite(normalisedTrainDF).all(1)] #Testing commenting this out\n",
    "\n",
    "print(\"Linear Train Accuracy: \",metrics.accuracy_score(trainLabels,y_pred))\n",
    "print(\"Poly Train Accuracy: \",metrics.accuracy_score(trainLabels,svmPolyModelPrediction))\n",
    "print(\"RBF Train Accuracy: \",metrics.accuracy_score(trainLabels,svmRBFModelPrediction))\n",
    "print(\"Sigmoid Train Accuracy: \",metrics.accuracy_score(trainLabels,svmSigmoidModelPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34661fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accuracy for the Validation Set\n",
    "\n",
    "svmValidationDatasetPrediction = svmModelPredict(model, normalisedValidDF)\n",
    "#svmValidationPrediction = model.predict(normalisedValidDF)\n",
    "print(\"Linear Valid Accuracy: \",metrics.accuracy_score(validLabels,svmValidationDatasetPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b5b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for the Testing Set\n",
    "\n",
    "svmTestDatasetPrediction = svmValidationPrediction = svmModelPredict(model, normalisedTestDF)\n",
    "#svmTestPrediction = model.predict(normalisedTestDF)\n",
    "print(\"Test Accuracy: \",metrics.accuracy_score(testLabels,svmTestDatasetPrediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cedd1de",
   "metadata": {},
   "source": [
    "## Confusion Matrix for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3022aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix Plotted and Printed Here\n",
    "\n",
    "ax = plt.subplot()\n",
    "predictResults = model.predict(normalisedTestDF)\n",
    "cm = confusion_matrix(predictResults,predictResults)\n",
    "\n",
    "ax.set_xlabel(\"Predicted Labels\")\n",
    "ax.set_ylabel(\"True Labels\")\n",
    "ax.set_title(\"Confusion Matrix - Linear\")\n",
    "ax.set_xticks([1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "sns.heatmap(cm, annot=True, ax=ax, yticklabels=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"], xticklabels=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]); #Semicolon removes the annoying text above the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report Details\n",
    "print(classification_report(testLabels,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de0ddda3",
   "metadata": {},
   "source": [
    "## Permutation importance stats for the model's weighting of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Importance Graphs Plotted and Printed Here - Compare these with how many files these instructions actually occur in\n",
    "\n",
    "permutationImportance = permutation_importance(model, normalisedTrainDF, trainLabels)\n",
    "featuresList = np.array(list(normalisedTrainDF.columns))\n",
    "sortedIDX = permutationImportance.importances_mean.argsort()\n",
    "mostImportantIndexesPermutation = [list(permutationImportance.importances_mean[sortedIDX]).index(i) for i in heapq.nlargest(30, permutationImportance.importances_mean[sortedIDX])]\n",
    "\n",
    "### Showing the largest features\n",
    "newFeaturesList = []\n",
    "newPermutationImportanceList = []\n",
    "\n",
    "for i in mostImportantIndexesPermutation[::-1]:\n",
    "    newFeaturesList.append(featuresList[sortedIDX][i])\n",
    "    newPermutationImportanceList.append(permutationImportance.importances_mean[sortedIDX][i])\n",
    "\n",
    "occurancesQuantity={}\n",
    "for i in newFeaturesList[::-1]:\n",
    "    occurancesQuantity.update({i:str(int(finalDF[i].mean()))})\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(newFeaturesList, newPermutationImportanceList);\n",
    "plt.xlabel(\"Permutation Importance/Feature\");\n",
    "plt.margins(x=0)\n",
    "plt.xticks([0,0.1,0.2,0.3,0.4,0.5],[\"0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"1\"])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(list(occurancesQuantity.keys())[::-1], preprocessing.minmax_scale(list(occurancesQuantity.values())[::-1],feature_range=(0,0.5)));\n",
    "plt.xlabel(\"Mean Relative occurances/Feature\");\n",
    "plt.xticks([0,0.1,0.2,0.3,0.4,0.5],[\"0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"1\"])\n",
    "plt.margins(x=0)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Classification Report stats graphed out here\n",
    "\n",
    "classificationReportDF = pd.DataFrame(classification_report(testLabels,y_pred,output_dict=True)).transpose()[:9]\n",
    "classificationReportF1Supp = classificationReportDF\n",
    "classificationReportF1Supp = classificationReportF1Supp[classificationReportF1Supp.columns[2:4]]\n",
    "classificationReportF1Supp[\"support\"] = classificationReportF1Supp[\"support\"].astype(int).div(100)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.bar(\n",
    "    x=classificationReportF1Supp.index.values.tolist(), \n",
    "    height=classificationReportF1Supp[\"f1-score\"], \n",
    "    width=0.5, \n",
    "    align='center')\n",
    "\n",
    "ax.bar(\n",
    "    x=classificationReportF1Supp.index.values.tolist(), \n",
    "    height=classificationReportF1Supp[\"support\"], \n",
    "    width=0.35, \n",
    "    align='center')\n",
    "\n",
    "f1ScoreBar = mpatches.Patch(color='blue', label=\"f1 score\")\n",
    "supportScoreBar = mpatches.Patch(color='orange', label=\"support\")\n",
    "ax.legend(handles=[f1ScoreBar, supportScoreBar],bbox_to_anchor=(0.5, -0.055), loc=\"upper center\",ncol=2)\n",
    "ax.set_title(\"A graph demonstrating the relationship between F1 scores and support\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
