{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "import heapq\n",
    "import codecs\n",
    "import json\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "from pathlib import Path #Convert all directory accesses to this\n",
    "from functools import reduce\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFullDirectory(mainDirectory,subDirectory):\n",
    "    #\n",
    "    # This function is used to concatenate two directories\n",
    "    #\n",
    "    return str(mainDirectory+subDirectory)\n",
    "\n",
    "def createFullPathToFile(fullDirectory, fileName):\n",
    "    #\n",
    "    # This function is used to concatenate a directory and filename\n",
    "    #\n",
    "    return str(fullDirectory+fileName)\n",
    "\n",
    "def listFilesInDirectory(directoryContainingFiles):\n",
    "    #\n",
    "    # This returns a list() of the names of files in a directory\n",
    "    #\n",
    "    return glob.glob(directoryContainingFiles) \n",
    "\n",
    "def stripFilePathAndExtension(filePath):\n",
    "    #\n",
    "    # This returns the filename without the path and extension\n",
    "    #\n",
    "    return Path(filePath).stem\n",
    "\n",
    "def replaceFilePathAndExtension(filePath, prefixToInsert, suffixToInsert):#filePath, prefixToStrip, prefixToInsert, suffixToStrip, suffixToInsert):\n",
    "    #\n",
    "    # This replaces the file path and extension with the given replacements\n",
    "    #\n",
    "    filePath = Path(filePath).stem\n",
    "    filePath = prefixToInsert+filePath+suffixToInsert\n",
    "    return filePath\n",
    "\n",
    "def printDataFrame(dataframe):\n",
    "    #\n",
    "    # This pretty prints a dataframe\n",
    "    #\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        print(dataframe)\n",
    "\n",
    "def zeroOutDataframe(dataframe):\n",
    "    #\n",
    "    # This fills null cells in a dataframe with zeros\n",
    "    #\n",
    "    dataframe = dataframe.fillna(0)\n",
    "    return dataframe\n",
    "\n",
    "def countEntriesInDataframe(dataframe):\n",
    "    #\n",
    "    # This counts the amount of non-zero cells in a dataframe\n",
    "    #\n",
    "    return np.count_nonzero(dataframe)\n",
    "\n",
    "def sortDictionary(dictionary):\n",
    "    #\n",
    "    # This sorts a dictionary in ascending order\n",
    "    #\n",
    "    returnVal = sorted(dict(Counter(dictionary)).items(), key=lambda kv:\n",
    "                 (kv[1], kv[0]))\n",
    "    return returnVal\n",
    "\n",
    "def fileNewlineIntoList(filePath):\n",
    "    #\n",
    "    # This takes a filepath as a parameter and returns the contents of the file in a list, seperated by newline\n",
    "    #\n",
    "    lineList = []\n",
    "    with open(filePath) as openFile:\n",
    "        for line in openFile:\n",
    "            temp = line.strip()\n",
    "            lineList.append(temp)\n",
    "    return lineList\n",
    "\n",
    "def stripNewlineAndWhitespace(textStringToStrip):\n",
    "    #\n",
    "    # This removes the whitespace and newlines from a string\n",
    "    #\n",
    "    textStringToStrip = textStringToStrip.replace(\"\\t\",\"\")\n",
    "    textStringToStrip = textStringToStrip.replace(\"\\n\",\"\")\n",
    "    textStringToStrip = textStringToStrip.replace(\" \",\"\")\n",
    "    return textStringToStrip\n",
    "\n",
    "def stripNewlineAndWhitespaceFromList(listToStrip):\n",
    "    #\n",
    "    # This removes the whitespace and newlines from every string in a list\n",
    "    #\n",
    "    for i in range(0,len(listToStrip)):\n",
    "        listToStrip[i] = stripNewlineAndWhitespace(listToStrip[i])\n",
    "    return listToStrip\n",
    "\n",
    "def regexSearchFile(filePath, regexPattern):\n",
    "    #\n",
    "    # This returns all of the matches in a file that match the given regex pattern\n",
    "    #\n",
    "    with open(filePath) as openFile:\n",
    "        matches = re.findall(regexPattern, openFile.read())\n",
    "    openFile.close()\n",
    "    return matches\n",
    "\n",
    "def cleanFileNameList(fileNameList,malwareClass, sortedDatasetDirectory): #NEED TO PORT THIS\n",
    "    #\n",
    "    # NEED TO FIX THIS\n",
    "    #\n",
    "    filePathToNameDict = {}\n",
    "    for i in range(0, len(fileNameList)): \n",
    "        strippedFile = stripFilePathAndExtension(fileNameList[i]) #FIX THIS TO ALLOW FOR DIFFERENT CLASSES\n",
    "        filePathToNameDict[strippedFile] = fileNameList[i]\n",
    "        fileNameList[i] = strippedFile\n",
    "    return fileNameList\n",
    "\n",
    "def generateClassDataFrame(listColumnsToUse,listRowsToUse):\n",
    "    #\n",
    "    # This creates an empty dataframe using two lists for the columns and index respectively\n",
    "    #\n",
    "    return zeroOutDataframe(pd.DataFrame(columns=listColumnsToUse,index=listRowsToUse))\n",
    "\n",
    "def moveFilesToClassFolders(backupFileList, fullFileNamesListFromCSV, unsortedDataset,sortedDataset):\n",
    "    #\n",
    "    # This reads the CSV mapping filenames to classes\n",
    "    # It iterates through the CSV's index to run an operation on every file\n",
    "    # The operation gets the filename alone and tries to copy it to it's folder\n",
    "    # The folder is determined by using a lookup of that clean filename in the index and checking the cell value of that index\n",
    "    #\n",
    "    fullFileNamesListFromCSV.set_index(\"Id\",inplace=True)\n",
    "    for fileIndex in range(0,len(backupFileList)): # file is the full path to the file, fileClean is just the name of the file without extension\n",
    "        fileClean = stripFilePathAndExtension(backupFileList[fileIndex])#,unsortedDataset,\".asm\") #This is to try fix an error\n",
    "        try:\n",
    "            shutil.copyfile(backupFileList[fileIndex],sortedDataset+\"class-\"+str(fullFileNamesListFromCSV.loc[fileClean,\"Class\"])+\"/\"+str(fullFileNamesListFromCSV.loc[fileClean].name)+\".asm\")\n",
    "        except:\n",
    "            fileIndex = fileIndex + 1\n",
    "\n",
    "def generateFilenameToDirectoryDict(listOfFiles):\n",
    "    #\n",
    "    # This takes a list of files in a directory\n",
    "    # It then creates a dictionary that maps the clean names to the paths\n",
    "    #\n",
    "    filePathToNameDict = {}\n",
    "    for file in listOfFiles:\n",
    "        filePathToNameDict[Path(file).stem] = file\n",
    "    return filePathToNameDict\n",
    "\n",
    "def populateMalwareDataframe(fileDirectoryTopLevel,instructionList):\n",
    "    #\n",
    "    # FILL THIS IN\n",
    "    #\n",
    "    filePathToNameDict = generateFilenameToDirectoryDict(listFilesInDirectory(fileDirectoryTopLevel))\n",
    "    dataFrame = zeroOutDataframe(pd.DataFrame(columns=instructionList,index=filePathToNameDict.keys()))\n",
    "\n",
    "    for file in filePathToNameDict.keys(): # Go through every file in our directory\n",
    "        fileDirectory = filePathToNameDict[file] # Convert using dict here\n",
    "        instructionsForThisFile = stripNewlineAndWhitespaceFromList(regexSearchFile(fileDirectory,\"(?:\\t{3,7}       (?!db|dd)[a-zA-Z]{2,6} {1,})\")) # cleaning and pulling instructions\n",
    "\n",
    "        pandasSeriesTest = pd.Series(instructionsForThisFile).value_counts().index, pd.Series(instructionsForThisFile).value_counts().values # Counting each instruction up   \n",
    "        for i in range(0, len(pandasSeriesTest[0])):\n",
    "            dataFrame.loc[file,pandasSeriesTest[0][i]] = pandasSeriesTest[1][i]  #0 = instruction and 1 = count columns ||| Second value is index within that column\n",
    "        \n",
    "        #Optional cleaning options for my DF to merge dupe columns and group them up\n",
    "        dataFrame = dataFrame.groupby(axis=1, level=0).sum() # Merges dupe columns\n",
    "        #dataFrame = dataFrame.loc[:, (dataFrame != 0).any(axis=0)] # Removes columns with no values\n",
    "    return dataFrame\n",
    "\n",
    "def classDataFrameCompletion(instructionList,sortedDataset,classList,classInteger):\n",
    "    #\n",
    "    # FILL THIS IN\n",
    "    #\n",
    "\n",
    "    #print(listFilesInDirectory(sortedDataset+classList[classInteger-1]+\"/*.asm\"))\n",
    "\n",
    "    \n",
    "    dataFrameInFunction = generateClassDataFrame(\n",
    "        instructionList,      # This is the instruction list\n",
    "        cleanFileNameList(    # This is the list of files\n",
    "            listFilesInDirectory(sortedDataset+classList[classInteger-1]+\"/*.asm\"),  # This is the directory containing the files\n",
    "            classInteger,\n",
    "            sortedDataset))  #This is the malware class for cleanFileNameList\n",
    "    \n",
    "    dataFrameInFunction = populateMalwareDataframe(\n",
    "                            sortedDataset+classList[classInteger-1]+\"/*.asm\",\n",
    "                            instructionList)\n",
    "\n",
    "    dataFrameInFunction = zeroOutDataframe(dataFrameInFunction)\n",
    "    \n",
    "    dataFrameInFunction.loc[~(dataFrameInFunction==0).all(axis=1)]\n",
    "    \n",
    "    dataFrameInFunction.insert(0,\"class\",classInteger)\n",
    "\n",
    "    #print(sortedDataset+classList[classInteger-1]+\"/*.asm\")\n",
    "\n",
    "    return dataFrameInFunction\n",
    "\n",
    "def removeNanValuesFromDataframe(dataframeToSanitise):\n",
    "    #\n",
    "    # This sanitises the DF be replacing NAN values with zero\n",
    "    #\n",
    "    dataframeToSanitise = dataframeToSanitise.replace(np.nan,0)\n",
    "    return dataframeToSanitise\n",
    "\n",
    "def normaliseData(dataframeToNormalise):\n",
    "    #\n",
    "    # This is the normalisation function that can be used to normalise data before it is fed into the model\n",
    "    #\n",
    "\n",
    "    #return (data -trainStats[\"mean\"]) / trainStats['std'] #Works fine, experimenting with the OTHER\n",
    "    #return data.div(data.sum(axis=1), axis=0)\n",
    "\n",
    "    dataframeToNormalise = removeNanValuesFromDataframe(dataframeToNormalise)\n",
    "    return dataframeToNormalise\n",
    "    \n",
    "def modelSVMClassifierCreate(cValue, kernelType):\n",
    "    #\n",
    "    # This returns an SVM model object using the C value and the kernel type\n",
    "    #\n",
    "    return svm.SVC(C=cValue, kernel=kernelType)\n",
    "    \n",
    "def svmModelFit(modelToFit,trainingDataframe, trainingDatasetLabels):\n",
    "    #\n",
    "    # This fits an SVM model to a dataset alongside the labels for training\n",
    "    #\n",
    "    return modelToFit.fit(trainingDataframe, trainingDatasetLabels)\n",
    "\n",
    "def svmModelPredict(modelForPrediction, dataframeToPredictWith):\n",
    "    #\n",
    "    # This takes a model and a dataframe to make predictions for each of the values in the dataframe\n",
    "    #\n",
    "    return modelForPrediction.predict(dataframeToPredictWith)\n",
    "\n",
    "def svmModelTrain(cValue, kernelType, trainingDataframe, trainingLabels):\n",
    "    #\n",
    "    # This creates and fits a model by taking the c value, kernel type and the training data\n",
    "    #\n",
    "    return svmModelFit(\n",
    "                modelSVMClassifierCreate(\n",
    "                    cValue, \n",
    "                    kernelType), \n",
    "                trainingDataframe, \n",
    "                trainingLabels)\n",
    "     \n",
    "def trainAndPredictModel(cValue, kernelType, trainingDataframe, trainingLabels):\n",
    "    #\n",
    "    # This trains the model using the previous functions and creates a prediction, returning the prediction and the model\n",
    "    #\n",
    "    model = modelSVMClassifierCreate(cValue, kernelType)\n",
    "    model = svmModelFit(model, trainingDataframe, trainingLabels)\n",
    "    modelPrediction = svmModelPredict(model, trainingDataframe)\n",
    "\n",
    "    print(kernelType+\" training accuracy: \",metrics.accuracy_score(trainingLabels,modelPrediction))\n",
    "\n",
    "    return modelPrediction, model\n",
    "\n",
    "def createSVMConfusionMatrix(predictResults, fileClassList):\n",
    "    #\n",
    "    # This creates a confusion matrix for a given set of results, with the fileClassList for the tick labels\n",
    "    #\n",
    "\n",
    "    for i in range(0, len(fileClassList)): fileClassList[i] = fileClassList[i][-1]#; fileClassList[i] = int(fileClassList[i])\n",
    "\n",
    "    ax = plt.subplot()\n",
    "    cm = confusion_matrix(predictResults,predictResults)\n",
    "\n",
    "    ax.set_xlabel(\"Predicted Labels\") # Doesn't work\n",
    "    ax.set_ylabel(\"True Labels\") # Doesn't work\n",
    "    ax.set_title(\"Confusion Matrix - Linear\")\n",
    "\n",
    "    sns.heatmap(cm, annot=True, ax=ax, yticklabels=fileClassList, xticklabels=fileClassList); #Semicolon removes the annoying text above the graph\n",
    "\n",
    "def classificationReportGenerateGraph(testLabels,svmTestDatasetPrediction):\n",
    "    #\n",
    "    # This creates a graph to show off the classification report for the model\n",
    "    #\n",
    "    classificationReportDF = pd.DataFrame(classification_report(testLabels,svmTestDatasetPrediction,output_dict=True)).transpose()[:9]\n",
    "    classificationReportF1Supp = classificationReportDF\n",
    "    classificationReportF1Supp = classificationReportF1Supp[classificationReportF1Supp.columns[2:4]]\n",
    "    classificationReportF1Supp[\"support\"] = classificationReportF1Supp[\"support\"].astype(int).div(100)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.bar(\n",
    "        x=classificationReportF1Supp.index.values.tolist(), \n",
    "        height=classificationReportF1Supp[\"f1-score\"], \n",
    "        width=0.5, \n",
    "        align='center')\n",
    "\n",
    "    ax.bar(\n",
    "        x=classificationReportF1Supp.index.values.tolist(), \n",
    "        height=classificationReportF1Supp[\"support\"], \n",
    "        width=0.35, \n",
    "        align='center')\n",
    "\n",
    "    f1ScoreBar = mpatches.Patch(color='blue', label=\"f1 score\")\n",
    "    supportScoreBar = mpatches.Patch(color='orange', label=\"support\")\n",
    "    ax.legend(handles=[f1ScoreBar, supportScoreBar],bbox_to_anchor=(0.5, -0.055), loc=\"upper center\",ncol=2)\n",
    "    ax.set_title(\"A graph demonstrating the relationship between F1 scores and support\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def permutationImportanceGraphPlot(model, normalisedTrainDF, trainLabels, finalDF):\n",
    "    #\n",
    "    # This takes the model and relevant data to derive the permutation importance for a linear SVM Model\n",
    "    #\n",
    "    permutationImportance = permutation_importance(model, normalisedTrainDF, trainLabels)\n",
    "    featuresList = np.array(list(normalisedTrainDF.columns))\n",
    "    sortedIDX = permutationImportance.importances_mean.argsort()\n",
    "    mostImportantIndexesPermutation = [list(permutationImportance.importances_mean[sortedIDX]).index(i) for i in heapq.nlargest(30, permutationImportance.importances_mean[sortedIDX])]\n",
    "\n",
    "    ### Showing the largest features\n",
    "    newFeaturesList = []\n",
    "    newPermutationImportanceList = []\n",
    "\n",
    "    for i in mostImportantIndexesPermutation[::-1]:\n",
    "        newFeaturesList.append(featuresList[sortedIDX][i])\n",
    "        newPermutationImportanceList.append(permutationImportance.importances_mean[sortedIDX][i])\n",
    "\n",
    "    occurancesQuantity={}\n",
    "    for i in newFeaturesList[::-1]:\n",
    "        occurancesQuantity.update({i:str(int(finalDF[i].mean()))})\n",
    "\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(newFeaturesList, newPermutationImportanceList);\n",
    "    plt.xlabel(\"Permutation Importance/Feature\");\n",
    "    plt.margins(x=0)\n",
    "    plt.xticks([0,0.1,0.2,0.3,0.4,0.5],[\"0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"1\"])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(list(occurancesQuantity.keys())[::-1], preprocessing.minmax_scale(list(occurancesQuantity.values())[::-1],feature_range=(0,0.5)));\n",
    "    plt.xlabel(\"Mean Relative occurances/Feature\");\n",
    "    plt.xticks([0,0.1,0.2,0.3,0.4,0.5],[\"0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"1\"])\n",
    "    plt.margins(x=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def setupSanitisedDatasetSubset(sortedDataset, classList):\n",
    "    #\n",
    "    # This is used to limit the amount of files in a given class to 250 or less to maintian a manageable dataset\n",
    "    #\n",
    "    for fileClass in classList:\n",
    "        directory = str(sortedDataset+fileClass+\"/*\")\n",
    "        fileList = listFilesInDirectory(directory) #glob.glob(directory)\n",
    "\n",
    "        #print(fileClass)\n",
    "        #print(len(fileList))\n",
    "\n",
    "        i = 0\n",
    "        for i in range(0,len(fileList)):\n",
    "            if(i >= 250):\n",
    "                os.remove(fileList[i])\n",
    "        print(len(listFilesInDirectory(sortedDataset+classList[0]+\"/*.asm\")))\n",
    "\n",
    "def pickleSaveModel(model, modelSaveLocation):\n",
    "    #\n",
    "    # This serialises a model and saves it to the disk\n",
    "    #\n",
    "    pickle.dump(model, open(modelSaveLocation, \"wb\"))\n",
    "\n",
    "def pickleLoadModel(modelLoadLocation):\n",
    "    #\n",
    "    # This is used to read a serialised model and import a trained SVM model\n",
    "    #\n",
    "    return pickle.load(open(modelLoadLocation,\"rb\"))\n",
    "\n",
    "def collectTrainingData(workingDirectory, sortedDataset, classList, instructionList):\n",
    "    #\n",
    "    # This returns an array of the populated datasets for each of the nine classes\n",
    "    #\n",
    "    #instructionList = fileNewlineIntoList(workingDirectory+\"instructionListComplete.txt\")\n",
    "    dataframeClassOne = classDataFrameCompletion(instructionList, sortedDataset, classList, 1)\n",
    "    dataframeClassTwo = classDataFrameCompletion(instructionList, sortedDataset, classList, 2)\n",
    "    dataframeClassThree = classDataFrameCompletion(instructionList, sortedDataset, classList, 3)\n",
    "    dataframeClassFour = classDataFrameCompletion(instructionList, sortedDataset, classList, 4)\n",
    "    dataframeClassFive = classDataFrameCompletion(instructionList, sortedDataset, classList, 5)\n",
    "    dataframeClassSix = classDataFrameCompletion(instructionList, sortedDataset, classList, 6)\n",
    "    dataframeClassSeven = classDataFrameCompletion(instructionList, sortedDataset, classList, 7)\n",
    "    dataframeClassEight = classDataFrameCompletion(instructionList, sortedDataset, classList, 8)\n",
    "    dataframeClassNine = classDataFrameCompletion(instructionList, sortedDataset, classList, 9)\n",
    "\n",
    "    return [dataframeClassOne,dataframeClassTwo,dataframeClassThree,dataframeClassFour,dataframeClassFive,dataframeClassSix,dataframeClassSeven,dataframeClassEight,dataframeClassNine]\n",
    "\n",
    "def collectCustomTrainingData(workingDirectory, customTrainingFilesList, instructionList):\n",
    "    #\n",
    "    # EXPERIMENTAL\n",
    "    # This is an attempt to allow for additions to the training set in a modular way\n",
    "    #\n",
    "\n",
    "    classifyDF = zeroOutDataframe(pd.DataFrame(columns=instructionList,index=list(customTrainingFilesList)))\n",
    "\n",
    "    return classifyDF\n",
    "\n",
    "\n",
    "def constructFinalDF(dataframesList, pickleName):\n",
    "    #\n",
    "    # This combines all of the datasets in the dataframe list into one\n",
    "    # The function merges all of the duplicate columns, cleans any nan values and prints some basic information about the final dataframe\n",
    "    #\n",
    "    finalDF = pd.concat(dataframesList).drop_duplicates()\n",
    "    finalDF = zeroOutDataframe(finalDF)\n",
    "    finalDF.loc[~(finalDF==0).all(axis=1)]\n",
    "    finalDF = finalDF.loc[:, (finalDF != 0).any(axis=0)] # Removes columns with no values\n",
    "    finalDF = finalDF.sample(frac=1)\n",
    "    #finalDF.info()\n",
    "    #finalDF.to_pickle(\"./\"+pickleName+\".pickle\")\n",
    "    return finalDF\n",
    "\n",
    "def createTrainTestValidSplits(finalDF, trainTestSplitSize, testToValidRatio):\n",
    "    #\n",
    "    # This returns train, test and valid dataframes in ratio 60 : 20 : 20\n",
    "    #\n",
    "    trainDF, testAndValidDF = train_test_split(finalDF, test_size=trainTestSplitSize) # previously 0.4 for 40% test+valid\n",
    "    testDF, validDF = train_test_split(testAndValidDF, test_size=testToValidRatio) # previously 0.5 for 50-50 test-valid split of the original 40%\n",
    "\n",
    "    print(f\"Training Dataset rows and columns: {trainDF.shape}\")\n",
    "    print(f\"Test Dataset rows and columns: {testDF.shape}\")\n",
    "    print(f\"Validation Dataset rows and columns: {validDF.shape}\")\n",
    "    return trainDF, validDF, testDF\n",
    "\n",
    "def writeTrainingStats(trainDF, writeLocation):\n",
    "    #\n",
    "    # Creates some basic training statistics for the training dataset\n",
    "    #\n",
    "    trainStats = trainDF.describe()\n",
    "    trainStats.pop(\"class\")\n",
    "    trainStats = trainStats.transpose()\n",
    "    trainStats.to_csv(writeLocation)\n",
    "\n",
    "def createExamplePrediction(normalisedTestDF, model):\n",
    "    #\n",
    "    # This takes the first 10 entries in the test dataframe and gets predictions for them \n",
    "    #\n",
    "    exampleSubsetDF = normalisedTestDF[:10]\n",
    "    exampleResult = model.predict(exampleSubsetDF)\n",
    "    dataFrameExampleResult = pd.Series(list(exampleSubsetDF.index),index=exampleResult)\n",
    "    dataFrameExampleResult.head(10)\n",
    "    \n",
    "def createSetLabels(dataframe, nameOfDataframe):\n",
    "    #\n",
    "    # Given a dataframe, this function can create a set of labels by removing them from the original dataframe\n",
    "    #\n",
    "    dataframeClone = dataframe\n",
    "    createdLabels = dataframeClone.pop(\"class\")\n",
    "    print(nameOfDataframe+\" data rows: \"+str(len(createdLabels)))\n",
    "    return createdLabels, dataframeClone\n",
    "\n",
    "def stripInstructions(filePath):\n",
    "    #\n",
    "    # POSSIBLE EXPERIMENTAL\n",
    "    # This function is used to pull all of the assembly instructions from a file\n",
    "    # Possible not needed anymore with the objdump development \n",
    "    #\n",
    "    pattern = re.compile(\"(?:^[a-zA-Z]{2,6}\\s)\")\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    for line in enumerate(open(filePath)):\n",
    "        for match in re.finditer(pattern, line):\n",
    "            currentMatch = match.group()\n",
    "            currentMatch = currentMatch.strip()\n",
    "            matches.append(currentMatch)\n",
    "\n",
    "def setupModelTrainingData(sortedDataset, instructionList, classList, workingDirectory, unsortedDataset, baseDirectory, customDataset, writeDFsToPickle, writeFinalDFToPickle, writeCustomFinalDFToPickle, readFinalDFFromPickle):\n",
    "    #\n",
    "    # This is the complete function to move all of the asm training data into the right place\n",
    "    # It then copies the custom dataset into the training set\n",
    "    # it splits the dataset into the relevant splits\n",
    "    # It pickles the three dataframes after normalising then\n",
    "    #\n",
    "    #moveFilesToClassFolders(listFilesInDirectory(unsortedDataset+\"*\"),pd.read_csv(baseDirectory+\"trainLabels.csv\"),unsortedDataset,sortedDataset)\n",
    "    \n",
    "    setupSanitisedDatasetSubset(sortedDataset, classList)\n",
    "    #shutil.copytree(customDataset,sortedDataset,dirs_exist_ok=True)\n",
    "    \n",
    "    if(readFinalDFFromPickle == True):\n",
    "        finalDF = pd.read_pickle(workingDirectory+\"finalDF.pickle\")\n",
    "        finalDFCustom = pd.read_pickle(workingDirectory+\"finalDFCustom.pickle\")\n",
    "    else:\n",
    "        dataframesList = collectTrainingData(workingDirectory,sortedDataset,classList, instructionList)\n",
    "        finalDF = constructFinalDF(dataframesList, \"finalDF\")\n",
    "        dataframesListCustom = collectTrainingData(workingDirectory,customDataset,classList, instructionList)\n",
    "        finalDFCustom = constructFinalDF(dataframesListCustom, \"finalDFCustom\")\n",
    "\n",
    "        if(writeFinalDFToPickle == True):\n",
    "            finalDF.to_pickle(workingDirectory+\"finalDF.pickle\")\n",
    "    \n",
    "        if(writeCustomFinalDFToPickle == True):\n",
    "            finalDFCustom.to_pickle(workingDirectory+\"finalDFCustom.pickle\")\n",
    "\n",
    "    trainDF, validDF, testDF = createTrainTestValidSplits(finalDF, float(0.3), float(0.1))\n",
    "\n",
    "    normalisedTrainDF = normaliseData(trainDF)\n",
    "    normalisedValidDF = normaliseData(validDF)\n",
    "    normalisedTestDF = normaliseData(testDF)\n",
    "\n",
    "    if(writeDFsToPickle == True):\n",
    "        normalisedTrainDF.to_pickle(workingDirectory+\"trainDF.pickle\")\n",
    "        normalisedValidDF.to_pickle(workingDirectory+\"validDF.pickle\")\n",
    "        normalisedTestDF.to_pickle(workingDirectory+\"testDF.pickle\")\n",
    "\n",
    "    \n",
    "\n",
    "    return normalisedTrainDF, normalisedValidDF, normalisedTestDF, finalDF, finalDFCustom\n",
    "\n",
    "def loadModelTrainingDataFromPickle(workingDirectory):\n",
    "    #\n",
    "    # This loads the pickle files for the three types of dataframes\n",
    "    #\n",
    "    trainingData = pd.read_pickle(workingDirectory+\"trainDF.pickle\")\n",
    "    validationData = pd.read_pickle(workingDirectory+\"validDF.pickle\")\n",
    "    testingData = pd.read_pickle(workingDirectory+\"testDF.pickle\")\n",
    "    return trainingData, validationData, testingData\n",
    "\n",
    "def customPopulateMalwareClassifyDataframe(fileDirectoryTopLevel, instructionList, classList, trainDF):\n",
    "    #\n",
    "    # This is responsible for going through the custom malware dataset and populating an array of DFs with the information of each custom class of malware\n",
    "    #\n",
    "    customDFs = []\n",
    "    \n",
    "    cleanFileNameList = listFilesInDirectory(fileDirectoryTopLevel+\"/*\")\n",
    "    for x in range(0,len(cleanFileNameList)): cleanFileNameList[x] = stripFilePathAndExtension(cleanFileNameList[x])\n",
    "        #print(cleanFileNameList)\n",
    "    customDFs.append(zeroOutDataframe(generateClassDataFrame(trainDF.columns,cleanFileNameList)))\n",
    "\n",
    "    for file in listFilesInDirectory(fileDirectoryTopLevel+\"/*\"):\n",
    "        instructions = fileNewlineIntoList(file)\n",
    "        instructionQuantities = Counter(instructions)\n",
    "\n",
    "        for column in customDFs[-1].columns: #Need to count the amount of times this column name appears\n",
    "            #print(\"File: \"+file+\" has \"+str(instructionQuantities[column])+\" occurances of: \"+column)\n",
    "            customDFs[-1].loc[stripFilePathAndExtension(file),column] = instructionQuantities[column]\n",
    "    #customDFs[-1].insert(0, \"class\", int(classList[i][-1]))\n",
    "    return customDFs[0]\n",
    "\n",
    "def customPopulateMalwareTrainingDataframe(fileDirectoryTopLevel,instructionList, classList, trainDF):\n",
    "    #\n",
    "    # This is responsible for going through the custom malware dataset and populating an array of DFs with the information of each custom class of malware\n",
    "    #\n",
    "    customDFs = []\n",
    "    for i in range(0,len(classList)):\n",
    "        cleanFileNameList = listFilesInDirectory(fileDirectoryTopLevel+classList[i]+\"/*\")\n",
    "        for x in range(0,len(cleanFileNameList)): cleanFileNameList[x] = stripFilePathAndExtension(cleanFileNameList[x])\n",
    "        customDFs.append(zeroOutDataframe(generateClassDataFrame(trainDF.columns,cleanFileNameList)))\n",
    "\n",
    "        for file in listFilesInDirectory(fileDirectoryTopLevel+classList[i]+\"/*\"):\n",
    "            #print(file+\" with index \"+str(i))\n",
    "            instructions = fileNewlineIntoList(file)\n",
    "            instructionQuantities = Counter(instructions)\n",
    "\n",
    "            for column in customDFs[-1].columns: #Need to count the amount of times this column name appears\n",
    "                #print(\"File: \"+file+\" has \"+str(instructionQuantities[column])+\" occurances of: \"+column)\n",
    "                customDFs[-1].loc[stripFilePathAndExtension(file),column] = instructionQuantities[column]\n",
    "        customDFs[-1].insert(0, \"class\", int(classList[i][-1]))\n",
    "    \n",
    "    combinedDF = customDFs[0]\n",
    "    for i in range(1,8): combinedDF = combinedDF.append(customDFs[i])            \n",
    "    return combinedDF\n",
    "\n",
    "def customClassDataFrameCompletion(instructionList,sortedDataset,classList,classInteger):\n",
    "    #\n",
    "    # This function is used to create a dataframe using the custom dataset that allows the user to add in their own malicious samples\n",
    "    # This takes the instruction list to generate the column names\n",
    "    # The sorted dataset is passed to the function to provide a path on the filesystem that the program can find the training data in\n",
    "    # The classlist is used to iterate through, and the classInteger is used to \n",
    "    #\n",
    "    #print(listFilesInDirectory(sortedDataset+classList[classInteger-1]+\"/*.asm\"))\n",
    "\n",
    "    dataFrameInFunction = generateClassDataFrame(\n",
    "        instructionList,      # This is the instruction list that is used to generate column names\n",
    "        cleanFileNameList(    # This is the list of files that matches the 3 following parameters\n",
    "            listFilesInDirectory(sortedDataset+classList[classInteger-1]+\"/*.asm\"),  # This is the directory containing the files for this specific class, it returns a list of the files in that class directory\n",
    "            classInteger,    #This designates the class of these malware samples\n",
    "            sortedDataset))  #This is the malware class for cleanFileNameList\n",
    "    \n",
    "    dataFrameInFunction = populateMalwareDataframe(     #This function populates the previously created dataframes using the passed sortedDataset path and the list of instructions\n",
    "                            sortedDataset+classList[classInteger-1]+\"/*.asm\",    #This is a formatted string that provides a wildcarded path to every assembly file in a given class\n",
    "                            instructionList)    # The instruction list is used to iterate over when physically populating the dataframe columns - In retrospect, pandas provides a way to pull dataframe.Columns straight out of the dataframe, making this redundant\n",
    "\n",
    "    dataFrameInFunction = zeroOutDataframe(dataFrameInFunction)     # This function is responsible for ensuring any values the model cannot handle, such as np.NaN or np.Inf, are changed to 0\n",
    "    \n",
    "    dataFrameInFunction.loc[~(dataFrameInFunction==0).all(axis=1)]  # This drops all rows with only zeroes in them\n",
    "    \n",
    "    dataFrameInFunction.insert(0,\"class\",classInteger)  #This creates a column in the dataframe consisting of an integer value of the malware class for the training labels\n",
    "\n",
    "    #print(sortedDataset+classList[classInteger-1]+\"/*.asm\")\n",
    "\n",
    "    return dataFrameInFunction\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingDirectory = os.getcwd()\n",
    "workingDirectory = workingDirectory+\"/\"\n",
    "baseDirectory = workingDirectory+\"data/\"\n",
    "\n",
    "unsortedDataset = createFullDirectory(baseDirectory,\"dataset-training-full-sanitised/\")\n",
    "sortedDataset = createFullDirectory(baseDirectory,\"dataset-training-subset-sorted/\")\n",
    "customDataset = createFullDirectory(baseDirectory,\"custom-dataset-sanitised/\")\n",
    "classifyFilesDataset = createFullDirectory(baseDirectory,\"filesToClassify/\")\n",
    "\n",
    "getMetrics = True\n",
    "\n",
    "classList = [\"class-1\",\"class-2\",\"class-3\",\"class-4\",\"class-5\",\"class-6\",\"class-7\",\"class-8\",\"class-9\"]\n",
    "instructionList = fileNewlineIntoList(workingDirectory+\"instructionListSubset.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "250\n",
      "250\n",
      "250\n",
      "250\n",
      "250\n",
      "250\n",
      "250\n",
      "250\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1811 entries, BKXtxeYlLsprabEWIQhn to HJayuX4Fh0GRBoALP6jx\n",
      "Columns: 349 entries, class to loopwe\n",
      "dtypes: float64(348), int64(1)\n",
      "memory usage: 4.8+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1 entries, CUSTOM_linux64 to CUSTOM_linux64\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   class   1 non-null      int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 16.0+ bytes\n",
      "Training Dataset rows and columns: (1267, 349)\n",
      "Test Dataset rows and columns: (489, 349)\n",
      "Validation Dataset rows and columns: (55, 349)\n"
     ]
    }
   ],
   "source": [
    "trainDF, validDF, testDF, finalDF, customFinalDF = setupModelTrainingData(sortedDataset, instructionList, classList, workingDirectory, unsortedDataset, baseDirectory, customDataset, True, True, True, False)\n",
    "#trainDF, validDF, testDF = setupModelTrainingData(sortedDataset, classList, workingDirectory, unsortedDataset, baseDirectory, customDataset, True, True, instructionList)\n",
    "#trainDF, validDF, testDF = loadModelTrainingDataFromPickle()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data data rows: 1267\n",
      "Valid data data rows: 55\n",
      "Test data data rows: 489\n",
      "linear training accuracy:  0.9984214680347278\n"
     ]
    }
   ],
   "source": [
    "trainLabels, trainDFWithoutLabels = createSetLabels(trainDF, \"Train data\")\n",
    "validLabels, validDFWithoutLabels = createSetLabels(validDF, \"Valid data\")\n",
    "testLabels, testDFWithoutLabels = createSetLabels(testDF, \"Test data\")\n",
    "\n",
    "\n",
    "svmTestDatasetPrediction, model = trainAndPredictModel(1.5, \"linear\", trainDFWithoutLabels, trainLabels)\n",
    "#svmPolyModelPrediction, modelPoly = trainAndPredictModel(1.5, \"poly\", trainDF, trainLabels)\n",
    "#svmRBFModelPrediction, modelRBF = trainAndPredictModel(1.5, \"rbf\", trainDF, trainLabels)\n",
    "#svmSigmoidModelPrediction, modelSig = trainAndPredictModel(1.5, \"sigmoid\", trainDF, trainLabels)\n",
    "\n",
    "model = svmModelTrain(1.5, \"linear\", trainDFWithoutLabels, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Validation Accuracy:  0.9454545454545454\n",
      "Linear Test Accuracy:  0.9120654396728016\n"
     ]
    }
   ],
   "source": [
    "svmValidationDatasetPrediction = svmModelPredict(model, validDFWithoutLabels)\n",
    "print(\"Linear Validation Accuracy: \",metrics.accuracy_score(validLabels,svmValidationDatasetPrediction))\n",
    "svmTestDatasetPrediction = svmModelPredict(model, testDFWithoutLabels)\n",
    "print(\"Linear Test Accuracy: \",metrics.accuracy_score(testLabels,svmTestDatasetPrediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainDF, validDF, testDF = loadModelTrainingDataFromPickle(workingDirectory)\n",
    "trainingCustom = customPopulateMalwareTrainingDataframe(customDataset,instructionList, classList, trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingCustom = trainingCustom.append(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedDFTrainLabels, combinedDFTrain = createSetLabels(removeNanValuesFromDataframe(trainingCustom), \"Custom training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWithCustomDataset = svmModelTrain(1.5, \"linear\", combinedDFTrain, combinedDFTrainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDatasetForCustomModel = customPopulateMalwareClassifyDataframe(classifyFilesDataset,instructionList, classList, trainDF)\n",
    "test = svmModelPredict(modelWithCustomDataset, predictionDatasetForCustomModel)\n",
    "\n",
    "for predictionIndex in range(0,len(predictionDatasetForCustomModel.index.values)):\n",
    "    print(\"File {0} is class {1} malware\".format(predictionDatasetForCustomModel.index[predictionIndex], int(test[predictionIndex])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Program exits  here if metrics are not needed\n",
    "if(getMetrics == False):\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   The final dataframe is loaded from the pickle on the disk\n",
    "#with open(workingDirectory+\"finalDF.pickle\", \"rb\") as pickled_FinalDF:\n",
    "#    finalDF = pickle.load(pickled_FinalDF)\n",
    "\n",
    "#dataframesList = list(collectTrainingData(workingDirectory,sortedDataset,classList,instructionList))\n",
    "#testFinalDF = constructFinalDF(dataframesList, \"finalDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Accuracy is attained here using the default dataset\n",
    "\n",
    "svmTestDatasetPrediction = svmModelPredict(model, trainDF)\n",
    "print(\"Linear Train Accuracy: \",metrics.accuracy_score(trainLabels,svmTestDatasetPrediction))\n",
    "\n",
    "svmValidationDatasetPrediction = svmModelPredict(model, validDF)\n",
    "print(\"Linear Validation Accuracy: \",metrics.accuracy_score(validLabels,svmValidationDatasetPrediction))\n",
    "\n",
    "svmTestDatasetPrediction = svmModelPredict(model, testDF)\n",
    "print(\"Linear Test Accuracy: \",metrics.accuracy_score(testLabels,svmTestDatasetPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   A classification report is generated here, demonstrating the performance values of each class in the model\n",
    "\n",
    "print(classification_report(testLabels,svmTestDatasetPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Permutation Importance Graphs Plotted and Printed Here - Compare these with how many files these instructions actually occur in\n",
    "\n",
    "permutationImportanceGraphPlot(model, trainDF, trainLabels, finalDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Plotted and Printed Here\n",
    "\n",
    "createSVMConfusionMatrix(svmTestDatasetPrediction,classList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Classification Report stats graphed out here\n",
    "\n",
    "classificationReportGenerateGraph(testLabels, svmTestDatasetPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2, valid2, test2 = loadModelTrainingDataFromPickle(workingDirectory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
